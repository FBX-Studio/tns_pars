"""
–ö–æ–ª–ª–µ–∫—Ç–æ—Ä –¥–ª—è –û–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–æ–≤ (OK.ru)
"""
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from config import Config
import logging
import time
import re

logger = logging.getLogger(__name__)

class OKCollector:
    """–ö–æ–ª–ª–µ–∫—Ç–æ—Ä –ø–æ—Å—Ç–æ–≤ –∏–∑ –û–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–æ–≤"""
    
    def __init__(self, sentiment_analyzer=None):
        self.keywords = Config.COMPANY_KEYWORDS
        self.sentiment_analyzer = sentiment_analyzer
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
        }
        
        # –ü—É–±–ª–∏—á–Ω—ã–µ –≥—Ä—É–ø–ø—ã –ù–∏–∂–Ω–µ–≥–æ –ù–æ–≤–≥–æ—Ä–æ–¥–∞
        self.public_groups = [
            'https://ok.ru/nizhnynovgorod',
            'https://ok.ru/nizhniynovgorod',
            'https://ok.ru/nnov',
            'https://ok.ru/nn52',
        ]
    
    def _is_relevant(self, text):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return False
        
        text_lower = text.lower()
        
        # –ò—Å–∫–ª—é—á–µ–Ω–∏—è
        exclude_patterns = [
            '–≥–∞–∑–ø—Ä–æ–º', '—Ç –ø–ª—é—Å', '—Ç-–ø–ª—é—Å',
            '—Ç–Ω—Å —Ç—É–ª–∞', '—Ç–Ω—Å —è—Ä–æ—Å–ª–∞–≤–ª—å',
            '–≤–∞–∫–∞–Ω—Å–∏—è', '—Ç—Ä–µ–±—É–µ—Ç—Å—è'
        ]
        
        for exclude in exclude_patterns:
            if exclude in text_lower:
                return False
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        if not self.keywords:
            return True
        
        for keyword in self.keywords:
            if keyword.lower() in text_lower:
                return True
        
        return False
    
    def search_ok_google(self, query):
        """–ü–æ–∏—Å–∫ –≤ OK —á–µ—Ä–µ–∑ Google –∏ DuckDuckGo"""
        posts = []
        
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            search_query = f'site:ok.ru {query}'
            
            # –ü—Ä–æ–±—É–µ–º –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo (–º–µ–Ω–µ–µ —Å—Ç—Ä–æ–≥–∏–π –∫ –±–æ—Ç–∞–º)
            ddg_url = f'https://html.duckduckgo.com/html/?q={requests.utils.quote(search_query)}'
            
            logger.info(f"[OK] –ü–æ–∏—Å–∫: {search_query}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º
            time.sleep(2)
            
            response = requests.get(ddg_url, headers=self.headers, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ OK –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö DuckDuckGo
                links = []
                for a in soup.find_all('a', href=True):
                    href = a['href']
                    # DuckDuckGo –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã
                    if 'uddg=' in href or 'ok.ru' in href:
                        try:
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–π URL
                            if 'uddg=' in href:
                                import urllib.parse
                                clean_url = urllib.parse.unquote(href.split('uddg=')[1].split('&')[0])
                            else:
                                clean_url = href
                            
                            if 'ok.ru' in clean_url and clean_url not in links:
                                # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å—Ç—ã –∏ —Ç–æ–ø–∏–∫–∏
                                if '/topic/' in clean_url or '/group/' in clean_url or 'tk=' in clean_url:
                                    links.append(clean_url)
                        except:
                            continue
                
                logger.info(f"[OK] –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(links)}")
                
                # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—ã–π –ø–æ—Å—Ç
                for url in links[:10]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10
                    try:
                        time.sleep(2)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                        post = self._parse_ok_post(url)
                        if post and self._is_relevant(post['text']):
                            logger.info(f"[OK] –ù–∞–π–¥–µ–Ω —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –ø–æ—Å—Ç: {post['text'][:50]}...")
                            posts.append(post)
                    except Exception as e:
                        logger.debug(f"[OK] –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
                        continue
            else:
                logger.warning(f"[OK] –ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–µ—Ä–Ω—É–ª–∞ –∫–æ–¥: {response.status_code}")
            
        except Exception as e:
            logger.error(f"[OK] –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        
        return posts
    
    def _parse_ok_post(self, url):
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø–æ—Å—Ç–∞ OK"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞
            text = ''
            text_block = soup.find('div', class_='media-text_cnt')
            if text_block:
                text = text_block.get_text(strip=True)
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ç–µ–∫—Å—Ç, –∏—â–µ–º –≤ –¥—Ä—É–≥–∏—Ö –º–µ—Å—Ç–∞—Ö
            if not text:
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤ –º–µ—Ç–∞-—Ç–µ–≥–∞—Ö
                meta_desc = soup.find('meta', {'property': 'og:description'})
                if meta_desc:
                    text = meta_desc.get('content', '')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–≤—Ç–æ—Ä–∞/–≥—Ä—É–ø–ø—É
            author = 'OK.ru'
            author_tag = soup.find('a', class_='mctc_name')
            if author_tag:
                author = author_tag.get_text(strip=True)
            else:
                # –ü—Ä–æ–±—É–µ–º –∏–∑ –º–µ—Ç–∞-—Ç–µ–≥–æ–≤
                meta_site = soup.find('meta', {'property': 'og:site_name'})
                if meta_site:
                    author = meta_site.get('content', 'OK.ru')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É
            pub_date = datetime.now()
            date_tag = soup.find('time')
            if date_tag:
                date_text = date_tag.get_text(strip=True)
                # –ü–∞—Ä—Å–∏–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞—Ç—ã ("2 —á–∞—Å–∞ –Ω–∞–∑–∞–¥", "–≤—á–µ—Ä–∞" –∏ —Ç.–¥.)
                try:
                    if '—á–∞—Å' in date_text:
                        hours = int(re.findall(r'\d+', date_text)[0])
                        from datetime import timedelta
                        pub_date = datetime.now() - timedelta(hours=hours)
                    elif '–¥–Ω' in date_text or '–¥–µ–Ω—å' in date_text:
                        days = int(re.findall(r'\d+', date_text)[0]) if re.findall(r'\d+', date_text) else 1
                        from datetime import timedelta
                        pub_date = datetime.now() - timedelta(days=days)
                except:
                    pass
            
            if not text:
                return None
            
            post_data = {
                'source': 'ok',
                'source_id': f"ok_{abs(hash(url))}",
                'author': author,
                'author_id': f"ok_{abs(hash(author))}",
                'text': text[:500],
                'url': url,
                'published_date': pub_date,
                'date': pub_date
            }
            
            # –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
            if self.sentiment_analyzer:
                sentiment = self.sentiment_analyzer.analyze(text)
                post_data['sentiment_score'] = sentiment['sentiment_score']
                post_data['sentiment_label'] = sentiment['sentiment_label']
            
            return post_data
            
        except Exception as e:
            logger.debug(f"[OK] –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø–æ—Å—Ç–∞: {e}")
            return None
    
    def collect_from_group(self, group_url):
        """–°–±–æ—Ä –ø–æ—Å—Ç–æ–≤ –∏–∑ –ø—É–±–ª–∏—á–Ω–æ–π –≥—Ä—É–ø–ø—ã —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º"""
        posts = []
        
        try:
            logger.info(f"[OK] –ü–∞—Ä—Å–∏–Ω–≥ –≥—Ä—É–ø–ø—ã: {group_url}")
            time.sleep(2)
            
            response = requests.get(group_url, headers=self.headers, timeout=15, allow_redirects=True)
            
            if response.status_code != 200:
                logger.warning(f"[OK] –ì—Ä—É–ø–ø–∞ –≤–µ—Ä–Ω—É–ª–∞ —Å—Ç–∞—Ç—É—Å {response.status_code}")
                return posts
            
            html_content = response.text
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–∞–ø—á—É
            if 'captcha' in html_content.lower():
                logger.warning(f"[OK] –ö–∞–ø—á–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –≥—Ä—É–ø–ø—ã {group_url}")
                return posts
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # –ò—â–µ–º –ø–æ—Å—Ç—ã –≤ –≥—Ä—É–ø–ø–µ (—Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤)
            post_links = []
            
            # –í–∞—Ä–∏–∞–Ω—Ç 1: –°—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–ø–∏–∫–∏
            for a in soup.find_all('a', href=True):
                href = a['href']
                if '/topic/' in href or 'st.topicId' in href:
                    full_url = f"https://ok.ru{href}" if href.startswith('/') else href
                    # –£–±–∏—Ä–∞–µ–º —è–∫–æ—Ä—è –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    clean_url = full_url.split('#')[0].split('?')[0]
                    if clean_url not in post_links and 'ok.ru' in clean_url:
                        post_links.append(clean_url)
                        logger.debug(f"[OK] –ù–∞–π–¥–µ–Ω —Ç–æ–ø–∏–∫ –≤ –≥—Ä—É–ø–ø–µ: {clean_url}")
            
            # –í–∞—Ä–∏–∞–Ω—Ç 2: Data-–∞—Ç—Ä–∏–±—É—Ç—ã
            for elem in soup.find_all(attrs={'data-topic-id': True}):
                topic_id = elem.get('data-topic-id')
                if topic_id:
                    url = f"https://ok.ru/topic/{topic_id}"
                    if url not in post_links:
                        post_links.append(url)
                        logger.debug(f"[OK] –ù–∞–π–¥–µ–Ω —Ç–æ–ø–∏–∫ —á–µ—Ä–µ–∑ data-–∞—Ç—Ä–∏–±—É—Ç: {url}")
            
            logger.info(f"[OK] –ù–∞–π–¥–µ–Ω–æ {len(post_links)} –ø–æ—Å—Ç–æ–≤ –≤ –≥—Ä—É–ø–ø–µ")
            
            # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—ã–π –ø–æ—Å—Ç
            for url in post_links[:10]:
                try:
                    time.sleep(2)
                    post = self._parse_ok_post(url)
                    if post and self._is_relevant(post['text']):
                        logger.info(f"[OK] –°–ø–∞—Ä—Å–µ–Ω –ø–æ—Å—Ç –∏–∑ –≥—Ä—É–ø–ø—ã: {post['text'][:60]}...")
                        posts.append(post)
                except Exception as e:
                    logger.debug(f"[OK] –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø–æ—Å—Ç–∞ {url}: {e}")
                    continue
            
        except Exception as e:
            logger.warning(f"[OK] –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≥—Ä—É–ø–ø—ã {group_url}: {e}")
            import traceback
            logger.debug(traceback.format_exc())
        
        return posts
    
    def search_ok_direct(self, query):
        """–ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –Ω–∞ OK.ru —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º"""
        posts = []
        
        try:
            # –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –Ω–∞ OK
            search_url = f'https://ok.ru/search?st.query={requests.utils.quote(query)}&st.mode=GlobalSearch'
            
            logger.info(f"[OK] –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –Ω–∞ OK.ru: {query}")
            time.sleep(2)
            
            response = requests.get(search_url, headers=self.headers, timeout=15, allow_redirects=True)
            
            logger.info(f"[OK] –û—Ç–≤–µ—Ç –æ—Ç OK.ru: —Å—Ç–∞—Ç—É—Å={response.status_code}, URL={response.url}")
            
            if response.status_code == 200:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                html_content = response.text
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∫–∞–ø—á—É
                if 'captcha' in html_content.lower() or '–ø—Ä–æ–≤–µ—Ä–∫–∞' in html_content.lower():
                    logger.warning("[OK] –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∫–∞–ø—á–∞! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–æ–∫—Å–∏ –∏–ª–∏ VPN")
                    return posts
                
                soup = BeautifulSoup(html_content, 'html.parser')
                
                # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–æ—Å—Ç—ã (—Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã)
                links = []
                
                # –í–∞—Ä–∏–∞–Ω—Ç 1: –°—Å—ã–ª–∫–∏ —Å /topic/
                for a in soup.find_all('a', href=True):
                    href = a['href']
                    if '/topic/' in href or '/dk?st.cmd=altGroupTopicInfo' in href or 'st.topicId' in href:
                        full_url = f"https://ok.ru{href}" if href.startswith('/') else href
                        if full_url not in links and 'ok.ru' in full_url:
                            links.append(full_url)
                            logger.debug(f"[OK] –ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞: {full_url}")
                
                # –í–∞—Ä–∏–∞–Ω—Ç 2: –ò—â–µ–º –≤ data-–∞—Ç—Ä–∏–±—É—Ç–∞—Ö
                for elem in soup.find_all(attrs={'data-topic-id': True}):
                    topic_id = elem.get('data-topic-id')
                    if topic_id:
                        url = f"https://ok.ru/topic/{topic_id}"
                        if url not in links:
                            links.append(url)
                            logger.debug(f"[OK] –ù–∞–π–¥–µ–Ω —Ç–æ–ø–∏–∫ –∏–∑ data-–∞—Ç—Ä–∏–±—É—Ç–∞: {url}")
                
                logger.info(f"[OK] –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫")
                
                # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, —Å–æ–∑–¥–∞–µ–º "—Ñ–µ–π–∫–æ–≤—ã–π" –ø–æ—Å—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ–∏—Å–∫–∞
                if len(links) == 0:
                    # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ö–æ—Ç—å –∫–∞–∫–æ–π-—Ç–æ —Ç–µ–∫—Å—Ç —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º
                    text_blocks = soup.find_all(['p', 'div', 'span'], string=lambda text: text and query.lower() in text.lower())
                    if text_blocks:
                        for block in text_blocks[:3]:
                            text = block.get_text(strip=True)
                            if len(text) > 20 and self._is_relevant(text):
                                logger.info(f"[OK] –ù–∞–π–¥–µ–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π –±–ª–æ–∫: {text[:100]}")
                                post_data = {
                                    'source': 'ok',
                                    'source_id': f"ok_search_{abs(hash(text))}",
                                    'author': '–û–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–∏ (–ø–æ–∏—Å–∫)',
                                    'author_id': 'ok_search',
                                    'text': text[:500],
                                    'url': search_url,
                                    'published_date': datetime.now(),
                                    'date': datetime.now()
                                }
                                if self.sentiment_analyzer:
                                    sentiment = self.sentiment_analyzer.analyze(text)
                                    post_data['sentiment_score'] = sentiment['sentiment_score']
                                    post_data['sentiment_label'] = sentiment['sentiment_label']
                                posts.append(post_data)
                    else:
                        logger.warning(f"[OK] –ü–æ –∑–∞–ø—Ä–æ—Å—É '{query}' –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                
                # –ü–∞—Ä—Å–∏–º –ø–æ—Å—Ç—ã –ø–æ —Å—Å—ã–ª–∫–∞–º
                for url in links[:5]:
                    try:
                        time.sleep(2)
                        post = self._parse_ok_post(url)
                        if post and self._is_relevant(post['text']):
                            logger.info(f"[OK] –°–ø–∞—Ä—Å–µ–Ω –ø–æ—Å—Ç: {post['text'][:80]}...")
                            posts.append(post)
                    except Exception as e:
                        logger.debug(f"[OK] –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
                        continue
            else:
                logger.warning(f"[OK] –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Å—Ç–∞—Ç—É—Å: {response.status_code}")
            
        except Exception as e:
            logger.error(f"[OK] –û—à–∏–±–∫–∞ –ø—Ä—è–º–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            import traceback
            logger.debug(traceback.format_exc())
        
        return posts
    
    def search_with_yandex(self, query):
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Yandex (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ DuckDuckGo)"""
        posts = []
        
        try:
            search_query = f'site:ok.ru {query} –ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥'
            yandex_url = f'https://yandex.ru/search/?text={requests.utils.quote(search_query)}'
            
            logger.info(f"[OK] –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Yandex: {search_query}")
            time.sleep(2)
            
            response = requests.get(yandex_url, headers=self.headers, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                links = []
                # –Ø–Ω–¥–µ–∫—Å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–ª–∞—Å—Å 'organic'
                for result in soup.find_all('li', class_='serp-item'):
                    link_tag = result.find('a', href=True)
                    if link_tag:
                        href = link_tag['href']
                        if 'ok.ru' in href and ('/topic/' in href or '/group/' in href):
                            if href not in links:
                                links.append(href)
                                logger.debug(f"[OK] –Ø–Ω–¥–µ–∫—Å –Ω–∞—à–µ–ª: {href}")
                
                logger.info(f"[OK] –Ø–Ω–¥–µ–∫—Å –Ω–∞—à–µ–ª {len(links)} —Å—Å—ã–ª–æ–∫")
                
                for url in links[:5]:
                    try:
                        time.sleep(2)
                        post = self._parse_ok_post(url)
                        if post and self._is_relevant(post['text']):
                            logger.info(f"[OK] –°–ø–∞—Ä—Å–µ–Ω –ø–æ—Å—Ç —á–µ—Ä–µ–∑ –Ø–Ω–¥–µ–∫—Å: {post['text'][:80]}...")
                            posts.append(post)
                    except Exception as e:
                        logger.debug(f"[OK] –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
                        continue
            
        except Exception as e:
            logger.error(f"[OK] –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ –Ø–Ω–¥–µ–∫—Å: {e}")
        
        return posts
    
    def collect(self):
        """–°–±–æ—Ä –ø–æ—Å—Ç–æ–≤ –∏–∑ –û–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–æ–≤"""
        all_posts = []
        
        try:
            logger.info("[OK] –ó–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞ –∏–∑ –û–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–æ–≤")
            logger.info("[OK] ‚ö† –í–ù–ò–ú–ê–ù–ò–ï: OK.ru –∞–∫—Ç–∏–≤–Ω–æ –±–ª–æ–∫–∏—Ä—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥")
            logger.info("[OK] –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–∫—Å–∏/VPN –∏–ª–∏ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π API")
            
            # 1. –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –Ω–∞ OK.ru (–Ω–∞–∏–±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π)
            for keyword in self.keywords[:2]:
                logger.info(f"[OK] –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É: {keyword}")
                try:
                    posts = self.search_ok_direct(keyword)
                    if posts:
                        logger.info(f"[OK] –ù–∞–π–¥–µ–Ω–æ {len(posts)} –ø–æ—Å—Ç–æ–≤ –ø–æ '{keyword}' (–ø—Ä—è–º–æ–π –ø–æ–∏—Å–∫)")
                        all_posts.extend(posts)
                    time.sleep(3)
                except Exception as e:
                    logger.warning(f"[OK] –û—à–∏–±–∫–∞ –ø—Ä—è–º–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ '{keyword}': {e}")
            
            # 2. –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –Ø–Ω–¥–µ–∫—Å (–ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ä—É—Å—Å–∫–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º)
            if len(all_posts) < 3:
                for keyword in self.keywords[:2]:
                    logger.info(f"[OK] –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –Ø–Ω–¥–µ–∫—Å –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É: {keyword}")
                    try:
                        posts = self.search_with_yandex(keyword)
                        if posts:
                            logger.info(f"[OK] –ù–∞–π–¥–µ–Ω–æ {len(posts)} –ø–æ—Å—Ç–æ–≤ –ø–æ '{keyword}' (–Ø–Ω–¥–µ–∫—Å)")
                            all_posts.extend(posts)
                        time.sleep(3)
                    except Exception as e:
                        logger.warning(f"[OK] –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ –Ø–Ω–¥–µ–∫—Å –ø–æ '{keyword}': {e}")
            
            # 3. –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥)
            if len(all_posts) < 3:
                for keyword in self.keywords[:2]:
                    logger.info(f"[OK] –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É: {keyword}")
                    try:
                        posts = self.search_ok_google(keyword)
                        if posts:
                            logger.info(f"[OK] –ù–∞–π–¥–µ–Ω–æ {len(posts)} –ø–æ—Å—Ç–æ–≤ –ø–æ '{keyword}' (DuckDuckGo)")
                            all_posts.extend(posts)
                        time.sleep(3)
                    except Exception as e:
                        logger.warning(f"[OK] –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ DuckDuckGo –ø–æ '{keyword}': {e}")
            
            # 4. –°–±–æ—Ä –∏–∑ –ø—É–±–ª–∏—á–Ω—ã—Ö –≥—Ä—É–ø–ø –ù–∏–∂–Ω–µ–≥–æ –ù–æ–≤–≥–æ—Ä–æ–¥–∞
            if len(all_posts) < 5:
                for group in self.public_groups[:2]:
                    logger.info(f"[OK] –°–±–æ—Ä –∏–∑ –≥—Ä—É–ø–ø—ã: {group}")
                    try:
                        posts = self.collect_from_group(group)
                        if posts:
                            logger.info(f"[OK] –ù–∞–π–¥–µ–Ω–æ {len(posts)} –ø–æ—Å—Ç–æ–≤ –≤ –≥—Ä—É–ø–ø–µ")
                            all_posts.extend(posts)
                        time.sleep(3)
                    except Exception as e:
                        logger.warning(f"[OK] –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –∏–∑ –≥—Ä—É–ø–ø—ã {group}: {e}")
            
            # –ò—Ç–æ–≥–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            if len(all_posts) == 0:
                logger.warning("[OK] ‚ö† –ü–æ—Å—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
                logger.warning("[OK] 1. OK.ru –±–ª–æ–∫–∏—Ä—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã –±–µ–∑ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")
                logger.warning("[OK] 2. –ù—É–∂–µ–Ω –ø—Ä–æ–∫—Å–∏/VPN (–Ω–∞—Å—Ç—Ä–æ–π—Ç–µ –≤ ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ‚Üí –ü—Ä–æ–∫—Å–∏)")
                logger.warning("[OK] 3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π API OK.ru: https://ok.ru/dk")
                logger.warning("[OK] 4. –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –ø–æ –≤–∞—à–∏–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –µ—Å—Ç—å –ø–æ—Å—Ç—ã –Ω–∞ OK.ru")
                logger.info("[OK] üí° –°–û–í–ï–¢: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Ä—É—á–Ω—É—é https://ok.ru/search?st.query=–¢–ù–°+—ç–Ω–µ—Ä–≥–æ+–ù–ù")
            else:
                logger.info(f"[OK] ‚úì –£—Å–ø–µ—à–Ω–æ –Ω–∞–π–¥–µ–Ω–æ {len(all_posts)} —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤!")
            
        except Exception as e:
            logger.error(f"[OK] –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞: {e}")
            import traceback
            logger.debug(traceback.format_exc())
        
        logger.info(f"[OK] –ò—Ç–æ–≥–æ —Å–æ–±—Ä–∞–Ω–æ –ø–æ—Å—Ç–æ–≤: {len(all_posts)}")
        return all_posts
