"""
–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ë–ï–ó –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
(VK –∏ –ù–æ–≤–æ—Å—Ç–Ω—ã–µ —Å–∞–π—Ç—ã - –æ–Ω–∏ –Ω–µ –±–ª–æ–∫–∏—Ä—É—é—Ç)
"""
import logging
import sys
import os
from dotenv import load_dotenv

load_dotenv(override=True)

from models import db, Review
from collectors.vk_collector import VKCollector
from collectors.news_collector import NewsCollector
from analyzers.sentiment_analyzer import SentimentAnalyzer
from analyzers.moderator import Moderator
from app import app
from datetime import datetime

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def main():
    logger.info("=" * 70)
    logger.info("–ë–ï–ó–û–ü–ê–°–ù–´–ô –°–ë–û–† –î–ê–ù–ù–´–•")
    logger.info("–ò—Å—Ç–æ—á–Ω–∏–∫–∏: VK + –ù–æ–≤–æ—Å—Ç–Ω—ã–µ —Å–∞–π—Ç—ã (–±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫)")
    logger.info("=" * 70)
    
    # –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–∫—Å–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    os.environ['USE_FREE_PROXIES'] = 'False'
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    sentiment_analyzer = SentimentAnalyzer()
    moderator = Moderator()
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ç–æ—Ä—ã –° sentiment_analyzer
    vk_collector = VKCollector()
    vk_collector.sentiment_analyzer = sentiment_analyzer
    
    news_collector = NewsCollector(sentiment_analyzer=sentiment_analyzer)
    news_collector.use_free_proxies = False
    news_collector.current_proxy = None
    
    all_reviews = []
    
    # 1. VK
    logger.info("\n1Ô∏è‚É£ –°–±–æ—Ä –∏–∑ VK...")
    try:
        vk_reviews = vk_collector.collect()
        logger.info(f"‚úì VK: –Ω–∞–π–¥–µ–Ω–æ {len(vk_reviews)} –æ—Ç–∑—ã–≤–æ–≤")
        all_reviews.extend(vk_reviews)
    except Exception as e:
        logger.error(f"‚úó –û—à–∏–±–∫–∞ VK: {e}")
    
    # 2. –ù–æ–≤–æ—Å—Ç–∏
    logger.info("\n2Ô∏è‚É£ –°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π...")
    try:
        news = news_collector.collect()
        logger.info(f"‚úì –ù–æ–≤–æ—Å—Ç–∏: –Ω–∞–π–¥–µ–Ω–æ {len(news)} —Å—Ç–∞—Ç–µ–π")
        all_reviews.extend(news)
    except Exception as e:
        logger.error(f"‚úó –û—à–∏–±–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    logger.info(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
    logger.info(f"–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(all_reviews)} –∑–∞–ø–∏—Å–µ–π")
    
    saved = 0
    parent_mapping = {}
    
    with app.app_context():
        for review_data in all_reviews:
            existing = Review.query.filter_by(source_id=review_data['source_id']).first()
            if existing:
                if not review_data.get('is_comment', False):
                    parent_mapping[review_data['source_id']] = existing.id
                continue
            
            sentiment = sentiment_analyzer.analyze(review_data['text'])
            keywords = sentiment_analyzer.extract_keywords(review_data['text'])
            
            moderation_status, moderation_reason, requires_manual = moderator.moderate(
                review_data['text'],
                sentiment['sentiment_score']
            )
            
            parent_id = None
            if review_data.get('is_comment', False):
                parent_source_id = review_data.get('parent_source_id')
                if parent_source_id:
                    parent_id = parent_mapping.get(parent_source_id)
                    if not parent_id:
                        parent = Review.query.filter_by(source_id=parent_source_id).first()
                        if parent:
                            parent_id = parent.id
                            parent_mapping[parent_source_id] = parent_id
            
            review = Review(
                source=review_data['source'],
                source_id=review_data['source_id'],
                author=review_data.get('author'),
                author_id=review_data.get('author_id'),
                text=review_data['text'],
                url=review_data.get('url'),
                published_date=review_data.get('published_date'),
                sentiment_score=sentiment['sentiment_score'],
                sentiment_label=sentiment['sentiment_label'],
                keywords=','.join(keywords) if keywords else None,
                moderation_status=moderation_status,
                moderation_reason=moderation_reason,
                requires_manual_review=requires_manual,
                processed=not requires_manual,
                parent_id=parent_id,
                is_comment=review_data.get('is_comment', False)
            )
            
            db.session.add(review)
            saved += 1
            
            if not review_data.get('is_comment', False):
                db.session.flush()
                parent_mapping[review_data['source_id']] = review.id
        
        db.session.commit()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total = Review.query.count()
        articles_count = Review.query.filter_by(is_comment=False).count()
        comments_count = Review.query.filter_by(is_comment=True).count()
        
        vk_count = Review.query.filter_by(source='vk', is_comment=False).count()
        news_count = Review.query.filter(Review.source.in_(['news', 'web']), Review.is_comment==False).count()
        
        logger.info("\n" + "=" * 70)
        logger.info("‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢–´")
        logger.info("=" * 70)
        logger.info(f"‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –Ω–æ–≤—ã—Ö: {saved}")
        logger.info(f"‚úì –í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {total} ({articles_count} —Å—Ç–∞—Ç–µ–π, {comments_count} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤)")
        logger.info(f"\n–ü–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")
        logger.info(f"  VK: {vk_count}")
        logger.info(f"  –ù–æ–≤–æ—Å—Ç–∏: {news_count}")
        
        logger.info("\nüí° –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:")
        logger.info("  Telegram, –î–∑–µ–Ω –∏ OK –ø—Ä–æ–ø—É—â–µ–Ω—ã –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π API")
        logger.info("  –°–º. DIAGNOSIS.md –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–µ–π")
        logger.info("=" * 70)

if __name__ == '__main__':
    main()
